{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22660\\3146107952.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(x*x), 2*x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397013\n",
      "Epoch 1, loss: 2.330839\n",
      "Epoch 2, loss: 2.310221\n",
      "Epoch 3, loss: 2.304235\n",
      "Epoch 4, loss: 2.302970\n",
      "Epoch 5, loss: 2.301240\n",
      "Epoch 6, loss: 2.303439\n",
      "Epoch 7, loss: 2.301806\n",
      "Epoch 8, loss: 2.301981\n",
      "Epoch 9, loss: 2.302028\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d00246acd0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.123\n",
      "Epoch 0, loss: 2.302074\n",
      "Epoch 1, loss: 2.301790\n",
      "Epoch 2, loss: 2.302027\n",
      "Epoch 3, loss: 2.301962\n",
      "Epoch 4, loss: 2.302027\n",
      "Epoch 5, loss: 2.301657\n",
      "Epoch 6, loss: 2.301891\n",
      "Epoch 7, loss: 2.301901\n",
      "Epoch 8, loss: 2.301054\n",
      "Epoch 9, loss: 2.301645\n",
      "Epoch 10, loss: 2.301260\n",
      "Epoch 11, loss: 2.301721\n",
      "Epoch 12, loss: 2.301979\n",
      "Epoch 13, loss: 2.302828\n",
      "Epoch 14, loss: 2.301723\n",
      "Epoch 15, loss: 2.301501\n",
      "Epoch 16, loss: 2.302176\n",
      "Epoch 17, loss: 2.302477\n",
      "Epoch 18, loss: 2.301875\n",
      "Epoch 19, loss: 2.302851\n",
      "Epoch 20, loss: 2.301717\n",
      "Epoch 21, loss: 2.301771\n",
      "Epoch 22, loss: 2.302264\n",
      "Epoch 23, loss: 2.301561\n",
      "Epoch 24, loss: 2.301596\n",
      "Epoch 25, loss: 2.301920\n",
      "Epoch 26, loss: 2.302158\n",
      "Epoch 27, loss: 2.302007\n",
      "Epoch 28, loss: 2.302555\n",
      "Epoch 29, loss: 2.302493\n",
      "Epoch 30, loss: 2.302212\n",
      "Epoch 31, loss: 2.301803\n",
      "Epoch 32, loss: 2.302261\n",
      "Epoch 33, loss: 2.302014\n",
      "Epoch 34, loss: 2.301479\n",
      "Epoch 35, loss: 2.301749\n",
      "Epoch 36, loss: 2.302101\n",
      "Epoch 37, loss: 2.302706\n",
      "Epoch 38, loss: 2.301938\n",
      "Epoch 39, loss: 2.301584\n",
      "Epoch 40, loss: 2.302705\n",
      "Epoch 41, loss: 2.302255\n",
      "Epoch 42, loss: 2.302769\n",
      "Epoch 43, loss: 2.302105\n",
      "Epoch 44, loss: 2.303017\n",
      "Epoch 45, loss: 2.301576\n",
      "Epoch 46, loss: 2.301927\n",
      "Epoch 47, loss: 2.301987\n",
      "Epoch 48, loss: 2.301193\n",
      "Epoch 49, loss: 2.302175\n",
      "Epoch 50, loss: 2.302222\n",
      "Epoch 51, loss: 2.302859\n",
      "Epoch 52, loss: 2.301147\n",
      "Epoch 53, loss: 2.302409\n",
      "Epoch 54, loss: 2.302440\n",
      "Epoch 55, loss: 2.301894\n",
      "Epoch 56, loss: 2.301428\n",
      "Epoch 57, loss: 2.302344\n",
      "Epoch 58, loss: 2.301657\n",
      "Epoch 59, loss: 2.301483\n",
      "Epoch 60, loss: 2.301973\n",
      "Epoch 61, loss: 2.301651\n",
      "Epoch 62, loss: 2.301336\n",
      "Epoch 63, loss: 2.302211\n",
      "Epoch 64, loss: 2.301579\n",
      "Epoch 65, loss: 2.301768\n",
      "Epoch 66, loss: 2.302362\n",
      "Epoch 67, loss: 2.302270\n",
      "Epoch 68, loss: 2.302178\n",
      "Epoch 69, loss: 2.302523\n",
      "Epoch 70, loss: 2.301919\n",
      "Epoch 71, loss: 2.301701\n",
      "Epoch 72, loss: 2.302455\n",
      "Epoch 73, loss: 2.302778\n",
      "Epoch 74, loss: 2.302288\n",
      "Epoch 75, loss: 2.301505\n",
      "Epoch 76, loss: 2.302568\n",
      "Epoch 77, loss: 2.301574\n",
      "Epoch 78, loss: 2.301295\n",
      "Epoch 79, loss: 2.301726\n",
      "Epoch 80, loss: 2.302110\n",
      "Epoch 81, loss: 2.301866\n",
      "Epoch 82, loss: 2.301387\n",
      "Epoch 83, loss: 2.302556\n",
      "Epoch 84, loss: 2.302886\n",
      "Epoch 85, loss: 2.302011\n",
      "Epoch 86, loss: 2.301322\n",
      "Epoch 87, loss: 2.301509\n",
      "Epoch 88, loss: 2.302708\n",
      "Epoch 89, loss: 2.301951\n",
      "Epoch 90, loss: 2.301569\n",
      "Epoch 91, loss: 2.301078\n",
      "Epoch 92, loss: 2.302599\n",
      "Epoch 93, loss: 2.301967\n",
      "Epoch 94, loss: 2.301890\n",
      "Epoch 95, loss: 2.300662\n",
      "Epoch 96, loss: 2.301721\n",
      "Epoch 97, loss: 2.301311\n",
      "Epoch 98, loss: 2.300970\n",
      "Epoch 99, loss: 2.303150\n",
      "Accuracy after training for 100 epochs:  0.119\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302108\n",
      "Epoch 1, loss: 2.300978\n",
      "Epoch 2, loss: 2.298565\n",
      "Epoch 3, loss: 2.296524\n",
      "Epoch 4, loss: 2.295413\n",
      "Epoch 5, loss: 2.294836\n",
      "Epoch 6, loss: 2.295870\n",
      "Epoch 7, loss: 2.297827\n",
      "Epoch 8, loss: 2.291598\n",
      "Epoch 9, loss: 2.296120\n",
      "Epoch 10, loss: 2.296082\n",
      "Epoch 11, loss: 2.293065\n",
      "Epoch 12, loss: 2.289324\n",
      "Epoch 13, loss: 2.292150\n",
      "Epoch 14, loss: 2.288675\n",
      "Epoch 15, loss: 2.291305\n",
      "Epoch 16, loss: 2.285007\n",
      "Epoch 17, loss: 2.287926\n",
      "Epoch 18, loss: 2.284230\n",
      "Epoch 19, loss: 2.284748\n",
      "Epoch 20, loss: 2.286946\n",
      "Epoch 21, loss: 2.283062\n",
      "Epoch 22, loss: 2.283947\n",
      "Epoch 23, loss: 2.279626\n",
      "Epoch 24, loss: 2.284460\n",
      "Epoch 25, loss: 2.285659\n",
      "Epoch 26, loss: 2.279039\n",
      "Epoch 27, loss: 2.279210\n",
      "Epoch 28, loss: 2.274641\n",
      "Epoch 29, loss: 2.283341\n",
      "Epoch 30, loss: 2.278239\n",
      "Epoch 31, loss: 2.278483\n",
      "Epoch 32, loss: 2.280101\n",
      "Epoch 33, loss: 2.276766\n",
      "Epoch 34, loss: 2.271577\n",
      "Epoch 35, loss: 2.277441\n",
      "Epoch 36, loss: 2.271227\n",
      "Epoch 37, loss: 2.277907\n",
      "Epoch 38, loss: 2.274241\n",
      "Epoch 39, loss: 2.271664\n",
      "Epoch 40, loss: 2.278085\n",
      "Epoch 41, loss: 2.272042\n",
      "Epoch 42, loss: 2.266297\n",
      "Epoch 43, loss: 2.265898\n",
      "Epoch 44, loss: 2.279775\n",
      "Epoch 45, loss: 2.272422\n",
      "Epoch 46, loss: 2.269812\n",
      "Epoch 47, loss: 2.269500\n",
      "Epoch 48, loss: 2.269948\n",
      "Epoch 49, loss: 2.269037\n",
      "Epoch 50, loss: 2.271655\n",
      "Epoch 51, loss: 2.263631\n",
      "Epoch 52, loss: 2.262797\n",
      "Epoch 53, loss: 2.258887\n",
      "Epoch 54, loss: 2.262597\n",
      "Epoch 55, loss: 2.247508\n",
      "Epoch 56, loss: 2.260037\n",
      "Epoch 57, loss: 2.253648\n",
      "Epoch 58, loss: 2.242671\n",
      "Epoch 59, loss: 2.262472\n",
      "Epoch 60, loss: 2.255230\n",
      "Epoch 61, loss: 2.270532\n",
      "Epoch 62, loss: 2.260979\n",
      "Epoch 63, loss: 2.258270\n",
      "Epoch 64, loss: 2.263647\n",
      "Epoch 65, loss: 2.263976\n",
      "Epoch 66, loss: 2.248059\n",
      "Epoch 67, loss: 2.260906\n",
      "Epoch 68, loss: 2.248155\n",
      "Epoch 69, loss: 2.255783\n",
      "Epoch 70, loss: 2.251087\n",
      "Epoch 71, loss: 2.257794\n",
      "Epoch 72, loss: 2.248725\n",
      "Epoch 73, loss: 2.262954\n",
      "Epoch 74, loss: 2.260854\n",
      "Epoch 75, loss: 2.257945\n",
      "Epoch 76, loss: 2.232517\n",
      "Epoch 77, loss: 2.266574\n",
      "Epoch 78, loss: 2.251340\n",
      "Epoch 79, loss: 2.247746\n",
      "Epoch 80, loss: 2.234660\n",
      "Epoch 81, loss: 2.254773\n",
      "Epoch 82, loss: 2.244422\n",
      "Epoch 83, loss: 2.262210\n",
      "Epoch 84, loss: 2.239807\n",
      "Epoch 85, loss: 2.248492\n",
      "Epoch 86, loss: 2.232402\n",
      "Epoch 87, loss: 2.242981\n",
      "Epoch 88, loss: 2.248048\n",
      "Epoch 89, loss: 2.213984\n",
      "Epoch 90, loss: 2.256448\n",
      "Epoch 91, loss: 2.240817\n",
      "Epoch 92, loss: 2.248809\n",
      "Epoch 93, loss: 2.225979\n",
      "Epoch 94, loss: 2.251240\n",
      "Epoch 95, loss: 2.232781\n",
      "Epoch 96, loss: 2.232244\n",
      "Epoch 97, loss: 2.253094\n",
      "Epoch 98, loss: 2.231626\n",
      "Epoch 99, loss: 2.228396\n",
      "Epoch 100, loss: 2.235436\n",
      "Epoch 101, loss: 2.243817\n",
      "Epoch 102, loss: 2.230024\n",
      "Epoch 103, loss: 2.229551\n",
      "Epoch 104, loss: 2.248007\n",
      "Epoch 105, loss: 2.222273\n",
      "Epoch 106, loss: 2.236848\n",
      "Epoch 107, loss: 2.239122\n",
      "Epoch 108, loss: 2.244184\n",
      "Epoch 109, loss: 2.254706\n",
      "Epoch 110, loss: 2.209704\n",
      "Epoch 111, loss: 2.231487\n",
      "Epoch 112, loss: 2.242841\n",
      "Epoch 113, loss: 2.256589\n",
      "Epoch 114, loss: 2.220862\n",
      "Epoch 115, loss: 2.237038\n",
      "Epoch 116, loss: 2.228485\n",
      "Epoch 117, loss: 2.228736\n",
      "Epoch 118, loss: 2.223818\n",
      "Epoch 119, loss: 2.220854\n",
      "Epoch 120, loss: 2.211154\n",
      "Epoch 121, loss: 2.226310\n",
      "Epoch 122, loss: 2.212449\n",
      "Epoch 123, loss: 2.209177\n",
      "Epoch 124, loss: 2.223051\n",
      "Epoch 125, loss: 2.223305\n",
      "Epoch 126, loss: 2.233390\n",
      "Epoch 127, loss: 2.225713\n",
      "Epoch 128, loss: 2.224406\n",
      "Epoch 129, loss: 2.206708\n",
      "Epoch 130, loss: 2.207375\n",
      "Epoch 131, loss: 2.222150\n",
      "Epoch 132, loss: 2.224165\n",
      "Epoch 133, loss: 2.220655\n",
      "Epoch 134, loss: 2.209922\n",
      "Epoch 135, loss: 2.224819\n",
      "Epoch 136, loss: 2.240099\n",
      "Epoch 137, loss: 2.215895\n",
      "Epoch 138, loss: 2.226781\n",
      "Epoch 139, loss: 2.225142\n",
      "Epoch 140, loss: 2.226731\n",
      "Epoch 141, loss: 2.223831\n",
      "Epoch 142, loss: 2.203263\n",
      "Epoch 143, loss: 2.233033\n",
      "Epoch 144, loss: 2.232306\n",
      "Epoch 145, loss: 2.226086\n",
      "Epoch 146, loss: 2.226323\n",
      "Epoch 147, loss: 2.222961\n",
      "Epoch 148, loss: 2.226807\n",
      "Epoch 149, loss: 2.232020\n",
      "Epoch 150, loss: 2.230779\n",
      "Epoch 151, loss: 2.221010\n",
      "Epoch 152, loss: 2.208751\n",
      "Epoch 153, loss: 2.206997\n",
      "Epoch 154, loss: 2.203202\n",
      "Epoch 155, loss: 2.205149\n",
      "Epoch 156, loss: 2.201224\n",
      "Epoch 157, loss: 2.237309\n",
      "Epoch 158, loss: 2.206707\n",
      "Epoch 159, loss: 2.202223\n",
      "Epoch 160, loss: 2.205621\n",
      "Epoch 161, loss: 2.214372\n",
      "Epoch 162, loss: 2.203159\n",
      "Epoch 163, loss: 2.189482\n",
      "Epoch 164, loss: 2.207618\n",
      "Epoch 165, loss: 2.201150\n",
      "Epoch 166, loss: 2.230997\n",
      "Epoch 167, loss: 2.202150\n",
      "Epoch 168, loss: 2.219967\n",
      "Epoch 169, loss: 2.203534\n",
      "Epoch 170, loss: 2.226569\n",
      "Epoch 171, loss: 2.208678\n",
      "Epoch 172, loss: 2.202350\n",
      "Epoch 173, loss: 2.221175\n",
      "Epoch 174, loss: 2.226059\n",
      "Epoch 175, loss: 2.223771\n",
      "Epoch 176, loss: 2.201716\n",
      "Epoch 177, loss: 2.225388\n",
      "Epoch 178, loss: 2.206198\n",
      "Epoch 179, loss: 2.211109\n",
      "Epoch 180, loss: 2.226243\n",
      "Epoch 181, loss: 2.195660\n",
      "Epoch 182, loss: 2.194490\n",
      "Epoch 183, loss: 2.201740\n",
      "Epoch 184, loss: 2.229600\n",
      "Epoch 185, loss: 2.222833\n",
      "Epoch 186, loss: 2.178338\n",
      "Epoch 187, loss: 2.193587\n",
      "Epoch 188, loss: 2.218807\n",
      "Epoch 189, loss: 2.203470\n",
      "Epoch 190, loss: 2.204454\n",
      "Epoch 191, loss: 2.216172\n",
      "Epoch 192, loss: 2.179730\n",
      "Epoch 193, loss: 2.205986\n",
      "Epoch 194, loss: 2.216469\n",
      "Epoch 195, loss: 2.205001\n",
      "Epoch 196, loss: 2.194767\n",
      "Epoch 197, loss: 2.188808\n",
      "Epoch 198, loss: 2.229836\n",
      "Epoch 199, loss: 2.195817\n",
      "Epoch 0, loss: 2.302757\n",
      "Epoch 1, loss: 2.301432\n",
      "Epoch 2, loss: 2.299395\n",
      "Epoch 3, loss: 2.299712\n",
      "Epoch 4, loss: 2.300412\n",
      "Epoch 5, loss: 2.297741\n",
      "Epoch 6, loss: 2.297404\n",
      "Epoch 7, loss: 2.295131\n",
      "Epoch 8, loss: 2.291710\n",
      "Epoch 9, loss: 2.294309\n",
      "Epoch 10, loss: 2.297269\n",
      "Epoch 11, loss: 2.295752\n",
      "Epoch 12, loss: 2.289085\n",
      "Epoch 13, loss: 2.293310\n",
      "Epoch 14, loss: 2.286796\n",
      "Epoch 15, loss: 2.284465\n",
      "Epoch 16, loss: 2.287174\n",
      "Epoch 17, loss: 2.287204\n",
      "Epoch 18, loss: 2.289105\n",
      "Epoch 19, loss: 2.279509\n",
      "Epoch 20, loss: 2.286220\n",
      "Epoch 21, loss: 2.284111\n",
      "Epoch 22, loss: 2.280016\n",
      "Epoch 23, loss: 2.285261\n",
      "Epoch 24, loss: 2.279224\n",
      "Epoch 25, loss: 2.282102\n",
      "Epoch 26, loss: 2.279715\n",
      "Epoch 27, loss: 2.283292\n",
      "Epoch 28, loss: 2.282148\n",
      "Epoch 29, loss: 2.272720\n",
      "Epoch 30, loss: 2.279138\n",
      "Epoch 31, loss: 2.279105\n",
      "Epoch 32, loss: 2.278807\n",
      "Epoch 33, loss: 2.279083\n",
      "Epoch 34, loss: 2.281264\n",
      "Epoch 35, loss: 2.277905\n",
      "Epoch 36, loss: 2.271946\n",
      "Epoch 37, loss: 2.279375\n",
      "Epoch 38, loss: 2.271260\n",
      "Epoch 39, loss: 2.269680\n",
      "Epoch 40, loss: 2.272639\n",
      "Epoch 41, loss: 2.268695\n",
      "Epoch 42, loss: 2.267485\n",
      "Epoch 43, loss: 2.269663\n",
      "Epoch 44, loss: 2.262298\n",
      "Epoch 45, loss: 2.264159\n",
      "Epoch 46, loss: 2.276484\n",
      "Epoch 47, loss: 2.272792\n",
      "Epoch 48, loss: 2.260142\n",
      "Epoch 49, loss: 2.261404\n",
      "Epoch 50, loss: 2.259333\n",
      "Epoch 51, loss: 2.253643\n",
      "Epoch 52, loss: 2.260066\n",
      "Epoch 53, loss: 2.268555\n",
      "Epoch 54, loss: 2.253440\n",
      "Epoch 55, loss: 2.245761\n",
      "Epoch 56, loss: 2.258434\n",
      "Epoch 57, loss: 2.262764\n",
      "Epoch 58, loss: 2.263476\n",
      "Epoch 59, loss: 2.256701\n",
      "Epoch 60, loss: 2.261247\n",
      "Epoch 61, loss: 2.266987\n",
      "Epoch 62, loss: 2.252017\n",
      "Epoch 63, loss: 2.250025\n",
      "Epoch 64, loss: 2.262952\n",
      "Epoch 65, loss: 2.250094\n",
      "Epoch 66, loss: 2.254000\n",
      "Epoch 67, loss: 2.264072\n",
      "Epoch 68, loss: 2.256628\n",
      "Epoch 69, loss: 2.248085\n",
      "Epoch 70, loss: 2.247558\n",
      "Epoch 71, loss: 2.256533\n",
      "Epoch 72, loss: 2.267081\n",
      "Epoch 73, loss: 2.248729\n",
      "Epoch 74, loss: 2.254586\n",
      "Epoch 75, loss: 2.269589\n",
      "Epoch 76, loss: 2.240176\n",
      "Epoch 77, loss: 2.244686\n",
      "Epoch 78, loss: 2.251290\n",
      "Epoch 79, loss: 2.248196\n",
      "Epoch 80, loss: 2.234375\n",
      "Epoch 81, loss: 2.250133\n",
      "Epoch 82, loss: 2.242799\n",
      "Epoch 83, loss: 2.245314\n",
      "Epoch 84, loss: 2.236505\n",
      "Epoch 85, loss: 2.246579\n",
      "Epoch 86, loss: 2.244908\n",
      "Epoch 87, loss: 2.234929\n",
      "Epoch 88, loss: 2.253255\n",
      "Epoch 89, loss: 2.233776\n",
      "Epoch 90, loss: 2.250923\n",
      "Epoch 91, loss: 2.228390\n",
      "Epoch 92, loss: 2.250085\n",
      "Epoch 93, loss: 2.245274\n",
      "Epoch 94, loss: 2.241133\n",
      "Epoch 95, loss: 2.248030\n",
      "Epoch 96, loss: 2.237224\n",
      "Epoch 97, loss: 2.244741\n",
      "Epoch 98, loss: 2.253814\n",
      "Epoch 99, loss: 2.233366\n",
      "Epoch 100, loss: 2.246266\n",
      "Epoch 101, loss: 2.241280\n",
      "Epoch 102, loss: 2.253457\n",
      "Epoch 103, loss: 2.252059\n",
      "Epoch 104, loss: 2.237966\n",
      "Epoch 105, loss: 2.238815\n",
      "Epoch 106, loss: 2.216265\n",
      "Epoch 107, loss: 2.247425\n",
      "Epoch 108, loss: 2.239010\n",
      "Epoch 109, loss: 2.241424\n",
      "Epoch 110, loss: 2.225515\n",
      "Epoch 111, loss: 2.240830\n",
      "Epoch 112, loss: 2.225064\n",
      "Epoch 113, loss: 2.234463\n",
      "Epoch 114, loss: 2.227842\n",
      "Epoch 115, loss: 2.209367\n",
      "Epoch 116, loss: 2.238662\n",
      "Epoch 117, loss: 2.218729\n",
      "Epoch 118, loss: 2.233303\n",
      "Epoch 119, loss: 2.229580\n",
      "Epoch 120, loss: 2.237892\n",
      "Epoch 121, loss: 2.224097\n",
      "Epoch 122, loss: 2.213373\n",
      "Epoch 123, loss: 2.214961\n",
      "Epoch 124, loss: 2.224985\n",
      "Epoch 125, loss: 2.216291\n",
      "Epoch 126, loss: 2.239344\n",
      "Epoch 127, loss: 2.223180\n",
      "Epoch 128, loss: 2.231314\n",
      "Epoch 129, loss: 2.217459\n",
      "Epoch 130, loss: 2.235901\n",
      "Epoch 131, loss: 2.236063\n",
      "Epoch 132, loss: 2.215533\n",
      "Epoch 133, loss: 2.216380\n",
      "Epoch 134, loss: 2.215758\n",
      "Epoch 135, loss: 2.203225\n",
      "Epoch 136, loss: 2.240708\n",
      "Epoch 137, loss: 2.217096\n",
      "Epoch 138, loss: 2.224308\n",
      "Epoch 139, loss: 2.229289\n",
      "Epoch 140, loss: 2.213374\n",
      "Epoch 141, loss: 2.221528\n",
      "Epoch 142, loss: 2.237182\n",
      "Epoch 143, loss: 2.214122\n",
      "Epoch 144, loss: 2.203051\n",
      "Epoch 145, loss: 2.224374\n",
      "Epoch 146, loss: 2.215684\n",
      "Epoch 147, loss: 2.208667\n",
      "Epoch 148, loss: 2.217730\n",
      "Epoch 149, loss: 2.225182\n",
      "Epoch 150, loss: 2.218159\n",
      "Epoch 151, loss: 2.216399\n",
      "Epoch 152, loss: 2.210451\n",
      "Epoch 153, loss: 2.210114\n",
      "Epoch 154, loss: 2.218592\n",
      "Epoch 155, loss: 2.224495\n",
      "Epoch 156, loss: 2.215611\n",
      "Epoch 157, loss: 2.223896\n",
      "Epoch 158, loss: 2.202900\n",
      "Epoch 159, loss: 2.216550\n",
      "Epoch 160, loss: 2.205124\n",
      "Epoch 161, loss: 2.209731\n",
      "Epoch 162, loss: 2.224130\n",
      "Epoch 163, loss: 2.211638\n",
      "Epoch 164, loss: 2.208134\n",
      "Epoch 165, loss: 2.234646\n",
      "Epoch 166, loss: 2.193706\n",
      "Epoch 167, loss: 2.213447\n",
      "Epoch 168, loss: 2.204057\n",
      "Epoch 169, loss: 2.219443\n",
      "Epoch 170, loss: 2.223973\n",
      "Epoch 171, loss: 2.205477\n",
      "Epoch 172, loss: 2.213108\n",
      "Epoch 173, loss: 2.212013\n",
      "Epoch 174, loss: 2.239442\n",
      "Epoch 175, loss: 2.219283\n",
      "Epoch 176, loss: 2.231849\n",
      "Epoch 177, loss: 2.232123\n",
      "Epoch 178, loss: 2.189870\n",
      "Epoch 179, loss: 2.208982\n",
      "Epoch 180, loss: 2.220751\n",
      "Epoch 181, loss: 2.216892\n",
      "Epoch 182, loss: 2.198661\n",
      "Epoch 183, loss: 2.212953\n",
      "Epoch 184, loss: 2.208977\n",
      "Epoch 185, loss: 2.224889\n",
      "Epoch 186, loss: 2.210479\n",
      "Epoch 187, loss: 2.207333\n",
      "Epoch 188, loss: 2.206215\n",
      "Epoch 189, loss: 2.188516\n",
      "Epoch 190, loss: 2.202432\n",
      "Epoch 191, loss: 2.204952\n",
      "Epoch 192, loss: 2.201690\n",
      "Epoch 193, loss: 2.192772\n",
      "Epoch 194, loss: 2.193102\n",
      "Epoch 195, loss: 2.201221\n",
      "Epoch 196, loss: 2.197109\n",
      "Epoch 197, loss: 2.196530\n",
      "Epoch 198, loss: 2.220137\n",
      "Epoch 199, loss: 2.211722\n",
      "Epoch 0, loss: 2.301556\n",
      "Epoch 1, loss: 2.298585\n",
      "Epoch 2, loss: 2.300562\n",
      "Epoch 3, loss: 2.298780\n",
      "Epoch 4, loss: 2.298210\n",
      "Epoch 5, loss: 2.296287\n",
      "Epoch 6, loss: 2.297744\n",
      "Epoch 7, loss: 2.297786\n",
      "Epoch 8, loss: 2.295659\n",
      "Epoch 9, loss: 2.294862\n",
      "Epoch 10, loss: 2.294820\n",
      "Epoch 11, loss: 2.298261\n",
      "Epoch 12, loss: 2.289706\n",
      "Epoch 13, loss: 2.286338\n",
      "Epoch 14, loss: 2.295141\n",
      "Epoch 15, loss: 2.286292\n",
      "Epoch 16, loss: 2.284095\n",
      "Epoch 17, loss: 2.280984\n",
      "Epoch 18, loss: 2.286966\n",
      "Epoch 19, loss: 2.287882\n",
      "Epoch 20, loss: 2.278953\n",
      "Epoch 21, loss: 2.288827\n",
      "Epoch 22, loss: 2.283088\n",
      "Epoch 23, loss: 2.283069\n",
      "Epoch 24, loss: 2.282517\n",
      "Epoch 25, loss: 2.279986\n",
      "Epoch 26, loss: 2.285652\n",
      "Epoch 27, loss: 2.284506\n",
      "Epoch 28, loss: 2.278609\n",
      "Epoch 29, loss: 2.282833\n",
      "Epoch 30, loss: 2.268369\n",
      "Epoch 31, loss: 2.273445\n",
      "Epoch 32, loss: 2.272125\n",
      "Epoch 33, loss: 2.277923\n",
      "Epoch 34, loss: 2.278297\n",
      "Epoch 35, loss: 2.273415\n",
      "Epoch 36, loss: 2.278060\n",
      "Epoch 37, loss: 2.268663\n",
      "Epoch 38, loss: 2.278641\n",
      "Epoch 39, loss: 2.263045\n",
      "Epoch 40, loss: 2.269426\n",
      "Epoch 41, loss: 2.261469\n",
      "Epoch 42, loss: 2.267409\n",
      "Epoch 43, loss: 2.276204\n",
      "Epoch 44, loss: 2.265268\n",
      "Epoch 45, loss: 2.261943\n",
      "Epoch 46, loss: 2.267088\n",
      "Epoch 47, loss: 2.264659\n",
      "Epoch 48, loss: 2.262027\n",
      "Epoch 49, loss: 2.265587\n",
      "Epoch 50, loss: 2.255664\n",
      "Epoch 51, loss: 2.267675\n",
      "Epoch 52, loss: 2.267785\n",
      "Epoch 53, loss: 2.269703\n",
      "Epoch 54, loss: 2.261133\n",
      "Epoch 55, loss: 2.255685\n",
      "Epoch 56, loss: 2.254168\n",
      "Epoch 57, loss: 2.247733\n",
      "Epoch 58, loss: 2.256009\n",
      "Epoch 59, loss: 2.261492\n",
      "Epoch 60, loss: 2.266169\n",
      "Epoch 61, loss: 2.257709\n",
      "Epoch 62, loss: 2.258625\n",
      "Epoch 63, loss: 2.250206\n",
      "Epoch 64, loss: 2.256646\n",
      "Epoch 65, loss: 2.268458\n",
      "Epoch 66, loss: 2.268632\n",
      "Epoch 67, loss: 2.257372\n",
      "Epoch 68, loss: 2.254349\n",
      "Epoch 69, loss: 2.242541\n",
      "Epoch 70, loss: 2.250042\n",
      "Epoch 71, loss: 2.268535\n",
      "Epoch 72, loss: 2.247233\n",
      "Epoch 73, loss: 2.268211\n",
      "Epoch 74, loss: 2.258516\n",
      "Epoch 75, loss: 2.257872\n",
      "Epoch 76, loss: 2.263775\n",
      "Epoch 77, loss: 2.238370\n",
      "Epoch 78, loss: 2.244799\n",
      "Epoch 79, loss: 2.249660\n",
      "Epoch 80, loss: 2.239271\n",
      "Epoch 81, loss: 2.239572\n",
      "Epoch 82, loss: 2.245828\n",
      "Epoch 83, loss: 2.249532\n",
      "Epoch 84, loss: 2.233386\n",
      "Epoch 85, loss: 2.241943\n",
      "Epoch 86, loss: 2.246630\n",
      "Epoch 87, loss: 2.245085\n",
      "Epoch 88, loss: 2.240928\n",
      "Epoch 89, loss: 2.249644\n",
      "Epoch 90, loss: 2.250558\n",
      "Epoch 91, loss: 2.240123\n",
      "Epoch 92, loss: 2.219718\n",
      "Epoch 93, loss: 2.239272\n",
      "Epoch 94, loss: 2.247834\n",
      "Epoch 95, loss: 2.236011\n",
      "Epoch 96, loss: 2.220323\n",
      "Epoch 97, loss: 2.227742\n",
      "Epoch 98, loss: 2.245895\n",
      "Epoch 99, loss: 2.231486\n",
      "Epoch 100, loss: 2.230421\n",
      "Epoch 101, loss: 2.242508\n",
      "Epoch 102, loss: 2.226704\n",
      "Epoch 103, loss: 2.225046\n",
      "Epoch 104, loss: 2.231341\n",
      "Epoch 105, loss: 2.240444\n",
      "Epoch 106, loss: 2.244193\n",
      "Epoch 107, loss: 2.245232\n",
      "Epoch 108, loss: 2.224177\n",
      "Epoch 109, loss: 2.215686\n",
      "Epoch 110, loss: 2.213893\n",
      "Epoch 111, loss: 2.231955\n",
      "Epoch 112, loss: 2.234266\n",
      "Epoch 113, loss: 2.237107\n",
      "Epoch 114, loss: 2.229869\n",
      "Epoch 115, loss: 2.218099\n",
      "Epoch 116, loss: 2.230799\n",
      "Epoch 117, loss: 2.233919\n",
      "Epoch 118, loss: 2.232089\n",
      "Epoch 119, loss: 2.240263\n",
      "Epoch 120, loss: 2.208141\n",
      "Epoch 121, loss: 2.241503\n",
      "Epoch 122, loss: 2.221180\n",
      "Epoch 123, loss: 2.243628\n",
      "Epoch 124, loss: 2.213264\n",
      "Epoch 125, loss: 2.216510\n",
      "Epoch 126, loss: 2.223385\n",
      "Epoch 127, loss: 2.228633\n",
      "Epoch 128, loss: 2.209744\n",
      "Epoch 129, loss: 2.213172\n",
      "Epoch 130, loss: 2.220055\n",
      "Epoch 131, loss: 2.212108\n",
      "Epoch 132, loss: 2.218818\n",
      "Epoch 133, loss: 2.204295\n",
      "Epoch 134, loss: 2.241948\n",
      "Epoch 135, loss: 2.222278\n",
      "Epoch 136, loss: 2.233529\n",
      "Epoch 137, loss: 2.222266\n",
      "Epoch 138, loss: 2.206095\n",
      "Epoch 139, loss: 2.230988\n",
      "Epoch 140, loss: 2.213006\n",
      "Epoch 141, loss: 2.187232\n",
      "Epoch 142, loss: 2.223845\n",
      "Epoch 143, loss: 2.242560\n",
      "Epoch 144, loss: 2.235505\n",
      "Epoch 145, loss: 2.198455\n",
      "Epoch 146, loss: 2.207364\n",
      "Epoch 147, loss: 2.221012\n",
      "Epoch 148, loss: 2.202237\n",
      "Epoch 149, loss: 2.227171\n",
      "Epoch 150, loss: 2.201816\n",
      "Epoch 151, loss: 2.217541\n",
      "Epoch 152, loss: 2.220382\n",
      "Epoch 153, loss: 2.222654\n",
      "Epoch 154, loss: 2.204679\n",
      "Epoch 155, loss: 2.222386\n",
      "Epoch 156, loss: 2.204049\n",
      "Epoch 157, loss: 2.218702\n",
      "Epoch 158, loss: 2.241845\n",
      "Epoch 159, loss: 2.188518\n",
      "Epoch 160, loss: 2.208131\n",
      "Epoch 161, loss: 2.189048\n",
      "Epoch 162, loss: 2.202935\n",
      "Epoch 163, loss: 2.229428\n",
      "Epoch 164, loss: 2.224192\n",
      "Epoch 165, loss: 2.220177\n",
      "Epoch 166, loss: 2.193358\n",
      "Epoch 167, loss: 2.218851\n",
      "Epoch 168, loss: 2.200192\n",
      "Epoch 169, loss: 2.198322\n",
      "Epoch 170, loss: 2.216089\n",
      "Epoch 171, loss: 2.208955\n",
      "Epoch 172, loss: 2.220474\n",
      "Epoch 173, loss: 2.193053\n",
      "Epoch 174, loss: 2.200134\n",
      "Epoch 175, loss: 2.217964\n",
      "Epoch 176, loss: 2.212540\n",
      "Epoch 177, loss: 2.213173\n",
      "Epoch 178, loss: 2.196474\n",
      "Epoch 179, loss: 2.227355\n",
      "Epoch 180, loss: 2.218458\n",
      "Epoch 181, loss: 2.203014\n",
      "Epoch 182, loss: 2.201246\n",
      "Epoch 183, loss: 2.206740\n",
      "Epoch 184, loss: 2.204469\n",
      "Epoch 185, loss: 2.231195\n",
      "Epoch 186, loss: 2.200313\n",
      "Epoch 187, loss: 2.201449\n",
      "Epoch 188, loss: 2.235688\n",
      "Epoch 189, loss: 2.211563\n",
      "Epoch 190, loss: 2.189465\n",
      "Epoch 191, loss: 2.200158\n",
      "Epoch 192, loss: 2.196801\n",
      "Epoch 193, loss: 2.220479\n",
      "Epoch 194, loss: 2.201099\n",
      "Epoch 195, loss: 2.199448\n",
      "Epoch 196, loss: 2.206642\n",
      "Epoch 197, loss: 2.213570\n",
      "Epoch 198, loss: 2.171408\n",
      "Epoch 199, loss: 2.168673\n",
      "Epoch 0, loss: 2.302916\n",
      "Epoch 1, loss: 2.303180\n",
      "Epoch 2, loss: 2.301847\n",
      "Epoch 3, loss: 2.302690\n",
      "Epoch 4, loss: 2.302607\n",
      "Epoch 5, loss: 2.301783\n",
      "Epoch 6, loss: 2.302788\n",
      "Epoch 7, loss: 2.301873\n",
      "Epoch 8, loss: 2.302622\n",
      "Epoch 9, loss: 2.303238\n",
      "Epoch 10, loss: 2.302849\n",
      "Epoch 11, loss: 2.300503\n",
      "Epoch 12, loss: 2.300984\n",
      "Epoch 13, loss: 2.302147\n",
      "Epoch 14, loss: 2.301492\n",
      "Epoch 15, loss: 2.300733\n",
      "Epoch 16, loss: 2.300793\n",
      "Epoch 17, loss: 2.300768\n",
      "Epoch 18, loss: 2.301092\n",
      "Epoch 19, loss: 2.301398\n",
      "Epoch 20, loss: 2.300095\n",
      "Epoch 21, loss: 2.301367\n",
      "Epoch 22, loss: 2.300405\n",
      "Epoch 23, loss: 2.301138\n",
      "Epoch 24, loss: 2.299583\n",
      "Epoch 25, loss: 2.301455\n",
      "Epoch 26, loss: 2.299206\n",
      "Epoch 27, loss: 2.298786\n",
      "Epoch 28, loss: 2.300864\n",
      "Epoch 29, loss: 2.301643\n",
      "Epoch 30, loss: 2.299812\n",
      "Epoch 31, loss: 2.299331\n",
      "Epoch 32, loss: 2.300410\n",
      "Epoch 33, loss: 2.299495\n",
      "Epoch 34, loss: 2.298332\n",
      "Epoch 35, loss: 2.300084\n",
      "Epoch 36, loss: 2.299395\n",
      "Epoch 37, loss: 2.296451\n",
      "Epoch 38, loss: 2.298267\n",
      "Epoch 39, loss: 2.299181\n",
      "Epoch 40, loss: 2.299138\n",
      "Epoch 41, loss: 2.298026\n",
      "Epoch 42, loss: 2.298610\n",
      "Epoch 43, loss: 2.298590\n",
      "Epoch 44, loss: 2.299578\n",
      "Epoch 45, loss: 2.298402\n",
      "Epoch 46, loss: 2.298091\n",
      "Epoch 47, loss: 2.300377\n",
      "Epoch 48, loss: 2.297326\n",
      "Epoch 49, loss: 2.297723\n",
      "Epoch 50, loss: 2.297677\n",
      "Epoch 51, loss: 2.298002\n",
      "Epoch 52, loss: 2.298567\n",
      "Epoch 53, loss: 2.295905\n",
      "Epoch 54, loss: 2.299360\n",
      "Epoch 55, loss: 2.296266\n",
      "Epoch 56, loss: 2.296568\n",
      "Epoch 57, loss: 2.296657\n",
      "Epoch 58, loss: 2.296870\n",
      "Epoch 59, loss: 2.297438\n",
      "Epoch 60, loss: 2.295293\n",
      "Epoch 61, loss: 2.294572\n",
      "Epoch 62, loss: 2.297685\n",
      "Epoch 63, loss: 2.295410\n",
      "Epoch 64, loss: 2.292231\n",
      "Epoch 65, loss: 2.298290\n",
      "Epoch 66, loss: 2.294884\n",
      "Epoch 67, loss: 2.297990\n",
      "Epoch 68, loss: 2.297283\n",
      "Epoch 69, loss: 2.296436\n",
      "Epoch 70, loss: 2.296506\n",
      "Epoch 71, loss: 2.299584\n",
      "Epoch 72, loss: 2.294679\n",
      "Epoch 73, loss: 2.298096\n",
      "Epoch 74, loss: 2.295938\n",
      "Epoch 75, loss: 2.294367\n",
      "Epoch 76, loss: 2.296949\n",
      "Epoch 77, loss: 2.295731\n",
      "Epoch 78, loss: 2.298326\n",
      "Epoch 79, loss: 2.292681\n",
      "Epoch 80, loss: 2.294576\n",
      "Epoch 81, loss: 2.293179\n",
      "Epoch 82, loss: 2.293328\n",
      "Epoch 83, loss: 2.295649\n",
      "Epoch 84, loss: 2.296719\n",
      "Epoch 85, loss: 2.294024\n",
      "Epoch 86, loss: 2.295769\n",
      "Epoch 87, loss: 2.293307\n",
      "Epoch 88, loss: 2.292060\n",
      "Epoch 89, loss: 2.292852\n",
      "Epoch 90, loss: 2.297055\n",
      "Epoch 91, loss: 2.297931\n",
      "Epoch 92, loss: 2.295805\n",
      "Epoch 93, loss: 2.296005\n",
      "Epoch 94, loss: 2.295313\n",
      "Epoch 95, loss: 2.296594\n",
      "Epoch 96, loss: 2.295440\n",
      "Epoch 97, loss: 2.296904\n",
      "Epoch 98, loss: 2.292176\n",
      "Epoch 99, loss: 2.291780\n",
      "Epoch 100, loss: 2.291861\n",
      "Epoch 101, loss: 2.296897\n",
      "Epoch 102, loss: 2.296742\n",
      "Epoch 103, loss: 2.291787\n",
      "Epoch 104, loss: 2.292367\n",
      "Epoch 105, loss: 2.291473\n",
      "Epoch 106, loss: 2.293276\n",
      "Epoch 107, loss: 2.288400\n",
      "Epoch 108, loss: 2.291422\n",
      "Epoch 109, loss: 2.290644\n",
      "Epoch 110, loss: 2.290370\n",
      "Epoch 111, loss: 2.295995\n",
      "Epoch 112, loss: 2.293529\n",
      "Epoch 113, loss: 2.290249\n",
      "Epoch 114, loss: 2.292905\n",
      "Epoch 115, loss: 2.288209\n",
      "Epoch 116, loss: 2.296461\n",
      "Epoch 117, loss: 2.292583\n",
      "Epoch 118, loss: 2.295605\n",
      "Epoch 119, loss: 2.294371\n",
      "Epoch 120, loss: 2.292028\n",
      "Epoch 121, loss: 2.288433\n",
      "Epoch 122, loss: 2.291421\n",
      "Epoch 123, loss: 2.285535\n",
      "Epoch 124, loss: 2.292111\n",
      "Epoch 125, loss: 2.289318\n",
      "Epoch 126, loss: 2.287403\n",
      "Epoch 127, loss: 2.290810\n",
      "Epoch 128, loss: 2.294054\n",
      "Epoch 129, loss: 2.292730\n",
      "Epoch 130, loss: 2.290243\n",
      "Epoch 131, loss: 2.297880\n",
      "Epoch 132, loss: 2.286868\n",
      "Epoch 133, loss: 2.289372\n",
      "Epoch 134, loss: 2.291860\n",
      "Epoch 135, loss: 2.288183\n",
      "Epoch 136, loss: 2.292253\n",
      "Epoch 137, loss: 2.291752\n",
      "Epoch 138, loss: 2.294825\n",
      "Epoch 139, loss: 2.292806\n",
      "Epoch 140, loss: 2.293595\n",
      "Epoch 141, loss: 2.288143\n",
      "Epoch 142, loss: 2.289079\n",
      "Epoch 143, loss: 2.295667\n",
      "Epoch 144, loss: 2.293482\n",
      "Epoch 145, loss: 2.285006\n",
      "Epoch 146, loss: 2.288980\n",
      "Epoch 147, loss: 2.291478\n",
      "Epoch 148, loss: 2.289620\n",
      "Epoch 149, loss: 2.291581\n",
      "Epoch 150, loss: 2.289958\n",
      "Epoch 151, loss: 2.291111\n",
      "Epoch 152, loss: 2.289972\n",
      "Epoch 153, loss: 2.285858\n",
      "Epoch 154, loss: 2.292715\n",
      "Epoch 155, loss: 2.284024\n",
      "Epoch 156, loss: 2.285947\n",
      "Epoch 157, loss: 2.289317\n",
      "Epoch 158, loss: 2.289223\n",
      "Epoch 159, loss: 2.285573\n",
      "Epoch 160, loss: 2.290815\n",
      "Epoch 161, loss: 2.291893\n",
      "Epoch 162, loss: 2.288898\n",
      "Epoch 163, loss: 2.286625\n",
      "Epoch 164, loss: 2.287376\n",
      "Epoch 165, loss: 2.288021\n",
      "Epoch 166, loss: 2.287449\n",
      "Epoch 167, loss: 2.287547\n",
      "Epoch 168, loss: 2.291383\n",
      "Epoch 169, loss: 2.282955\n",
      "Epoch 170, loss: 2.287456\n",
      "Epoch 171, loss: 2.285786\n",
      "Epoch 172, loss: 2.286770\n",
      "Epoch 173, loss: 2.283377\n",
      "Epoch 174, loss: 2.288613\n",
      "Epoch 175, loss: 2.285128\n",
      "Epoch 176, loss: 2.284598\n",
      "Epoch 177, loss: 2.289074\n",
      "Epoch 178, loss: 2.286001\n",
      "Epoch 179, loss: 2.282042\n",
      "Epoch 180, loss: 2.286905\n",
      "Epoch 181, loss: 2.286281\n",
      "Epoch 182, loss: 2.289138\n",
      "Epoch 183, loss: 2.287884\n",
      "Epoch 184, loss: 2.283574\n",
      "Epoch 185, loss: 2.286687\n",
      "Epoch 186, loss: 2.286886\n",
      "Epoch 187, loss: 2.289482\n",
      "Epoch 188, loss: 2.289702\n",
      "Epoch 189, loss: 2.283711\n",
      "Epoch 190, loss: 2.287566\n",
      "Epoch 191, loss: 2.287093\n",
      "Epoch 192, loss: 2.288083\n",
      "Epoch 193, loss: 2.283354\n",
      "Epoch 194, loss: 2.287483\n",
      "Epoch 195, loss: 2.284426\n",
      "Epoch 196, loss: 2.285954\n",
      "Epoch 197, loss: 2.282708\n",
      "Epoch 198, loss: 2.286985\n",
      "Epoch 199, loss: 2.281904\n",
      "Epoch 0, loss: 2.302733\n",
      "Epoch 1, loss: 2.301945\n",
      "Epoch 2, loss: 2.301974\n",
      "Epoch 3, loss: 2.301403\n",
      "Epoch 4, loss: 2.301238\n",
      "Epoch 5, loss: 2.301334\n",
      "Epoch 6, loss: 2.303029\n",
      "Epoch 7, loss: 2.301956\n",
      "Epoch 8, loss: 2.299944\n",
      "Epoch 9, loss: 2.301706\n",
      "Epoch 10, loss: 2.301947\n",
      "Epoch 11, loss: 2.301110\n",
      "Epoch 12, loss: 2.301352\n",
      "Epoch 13, loss: 2.299842\n",
      "Epoch 14, loss: 2.300187\n",
      "Epoch 15, loss: 2.300663\n",
      "Epoch 16, loss: 2.302016\n",
      "Epoch 17, loss: 2.299089\n",
      "Epoch 18, loss: 2.301499\n",
      "Epoch 19, loss: 2.299228\n",
      "Epoch 20, loss: 2.299999\n",
      "Epoch 21, loss: 2.297310\n",
      "Epoch 22, loss: 2.298764\n",
      "Epoch 23, loss: 2.300899\n",
      "Epoch 24, loss: 2.301049\n",
      "Epoch 25, loss: 2.297936\n",
      "Epoch 26, loss: 2.299917\n",
      "Epoch 27, loss: 2.299159\n",
      "Epoch 28, loss: 2.299505\n",
      "Epoch 29, loss: 2.299333\n",
      "Epoch 30, loss: 2.297903\n",
      "Epoch 31, loss: 2.298236\n",
      "Epoch 32, loss: 2.299408\n",
      "Epoch 33, loss: 2.298122\n",
      "Epoch 34, loss: 2.299994\n",
      "Epoch 35, loss: 2.298167\n",
      "Epoch 36, loss: 2.299624\n",
      "Epoch 37, loss: 2.298154\n",
      "Epoch 38, loss: 2.298324\n",
      "Epoch 39, loss: 2.300067\n",
      "Epoch 40, loss: 2.299020\n",
      "Epoch 41, loss: 2.298481\n",
      "Epoch 42, loss: 2.297950\n",
      "Epoch 43, loss: 2.296859\n",
      "Epoch 44, loss: 2.297398\n",
      "Epoch 45, loss: 2.296470\n",
      "Epoch 46, loss: 2.297720\n",
      "Epoch 47, loss: 2.298482\n",
      "Epoch 48, loss: 2.296469\n",
      "Epoch 49, loss: 2.295598\n",
      "Epoch 50, loss: 2.297352\n",
      "Epoch 51, loss: 2.296447\n",
      "Epoch 52, loss: 2.295345\n",
      "Epoch 53, loss: 2.297631\n",
      "Epoch 54, loss: 2.296236\n",
      "Epoch 55, loss: 2.296358\n",
      "Epoch 56, loss: 2.298301\n",
      "Epoch 57, loss: 2.299618\n",
      "Epoch 58, loss: 2.295060\n",
      "Epoch 59, loss: 2.297252\n",
      "Epoch 60, loss: 2.295989\n",
      "Epoch 61, loss: 2.297244\n",
      "Epoch 62, loss: 2.298112\n",
      "Epoch 63, loss: 2.295434\n",
      "Epoch 64, loss: 2.297154\n",
      "Epoch 65, loss: 2.295331\n",
      "Epoch 66, loss: 2.295381\n",
      "Epoch 67, loss: 2.296175\n",
      "Epoch 68, loss: 2.297898\n",
      "Epoch 69, loss: 2.297615\n",
      "Epoch 70, loss: 2.296098\n",
      "Epoch 71, loss: 2.294989\n",
      "Epoch 72, loss: 2.294876\n",
      "Epoch 73, loss: 2.293654\n",
      "Epoch 74, loss: 2.294777\n",
      "Epoch 75, loss: 2.295883\n",
      "Epoch 76, loss: 2.294896\n",
      "Epoch 77, loss: 2.293047\n",
      "Epoch 78, loss: 2.291381\n",
      "Epoch 79, loss: 2.296735\n",
      "Epoch 80, loss: 2.293490\n",
      "Epoch 81, loss: 2.295710\n",
      "Epoch 82, loss: 2.296444\n",
      "Epoch 83, loss: 2.294577\n",
      "Epoch 84, loss: 2.294786\n",
      "Epoch 85, loss: 2.294548\n",
      "Epoch 86, loss: 2.293041\n",
      "Epoch 87, loss: 2.292164\n",
      "Epoch 88, loss: 2.293687\n",
      "Epoch 89, loss: 2.297088\n",
      "Epoch 90, loss: 2.293032\n",
      "Epoch 91, loss: 2.295221\n",
      "Epoch 92, loss: 2.291935\n",
      "Epoch 93, loss: 2.295066\n",
      "Epoch 94, loss: 2.293267\n",
      "Epoch 95, loss: 2.289445\n",
      "Epoch 96, loss: 2.291511\n",
      "Epoch 97, loss: 2.294072\n",
      "Epoch 98, loss: 2.291215\n",
      "Epoch 99, loss: 2.293967\n",
      "Epoch 100, loss: 2.290303\n",
      "Epoch 101, loss: 2.292847\n",
      "Epoch 102, loss: 2.290917\n",
      "Epoch 103, loss: 2.292613\n",
      "Epoch 104, loss: 2.293504\n",
      "Epoch 105, loss: 2.291839\n",
      "Epoch 106, loss: 2.289583\n",
      "Epoch 107, loss: 2.290424\n",
      "Epoch 108, loss: 2.290728\n",
      "Epoch 109, loss: 2.294891\n",
      "Epoch 110, loss: 2.288782\n",
      "Epoch 111, loss: 2.294598\n",
      "Epoch 112, loss: 2.287929\n",
      "Epoch 113, loss: 2.290111\n",
      "Epoch 114, loss: 2.290780\n",
      "Epoch 115, loss: 2.288625\n",
      "Epoch 116, loss: 2.291286\n",
      "Epoch 117, loss: 2.287709\n",
      "Epoch 118, loss: 2.294292\n",
      "Epoch 119, loss: 2.290118\n",
      "Epoch 120, loss: 2.290770\n",
      "Epoch 121, loss: 2.287370\n",
      "Epoch 122, loss: 2.287685\n",
      "Epoch 123, loss: 2.292304\n",
      "Epoch 124, loss: 2.286673\n",
      "Epoch 125, loss: 2.289752\n",
      "Epoch 126, loss: 2.286044\n",
      "Epoch 127, loss: 2.289124\n",
      "Epoch 128, loss: 2.288512\n",
      "Epoch 129, loss: 2.287361\n",
      "Epoch 130, loss: 2.292833\n",
      "Epoch 131, loss: 2.290609\n",
      "Epoch 132, loss: 2.289085\n",
      "Epoch 133, loss: 2.288057\n",
      "Epoch 134, loss: 2.290743\n",
      "Epoch 135, loss: 2.290579\n",
      "Epoch 136, loss: 2.291378\n",
      "Epoch 137, loss: 2.291327\n",
      "Epoch 138, loss: 2.288345\n",
      "Epoch 139, loss: 2.287558\n",
      "Epoch 140, loss: 2.288969\n",
      "Epoch 141, loss: 2.289311\n",
      "Epoch 142, loss: 2.288367\n",
      "Epoch 143, loss: 2.287996\n",
      "Epoch 144, loss: 2.291065\n",
      "Epoch 145, loss: 2.290508\n",
      "Epoch 146, loss: 2.287623\n",
      "Epoch 147, loss: 2.288884\n",
      "Epoch 148, loss: 2.297461\n",
      "Epoch 149, loss: 2.288160\n",
      "Epoch 150, loss: 2.294472\n",
      "Epoch 151, loss: 2.288383\n",
      "Epoch 152, loss: 2.288261\n",
      "Epoch 153, loss: 2.288113\n",
      "Epoch 154, loss: 2.290747\n",
      "Epoch 155, loss: 2.293346\n",
      "Epoch 156, loss: 2.289032\n",
      "Epoch 157, loss: 2.287207\n",
      "Epoch 158, loss: 2.286803\n",
      "Epoch 159, loss: 2.287778\n",
      "Epoch 160, loss: 2.284669\n",
      "Epoch 161, loss: 2.287454\n",
      "Epoch 162, loss: 2.290918\n",
      "Epoch 163, loss: 2.289549\n",
      "Epoch 164, loss: 2.295185\n",
      "Epoch 165, loss: 2.290997\n",
      "Epoch 166, loss: 2.288647\n",
      "Epoch 167, loss: 2.288387\n",
      "Epoch 168, loss: 2.291815\n",
      "Epoch 169, loss: 2.286402\n",
      "Epoch 170, loss: 2.285238\n",
      "Epoch 171, loss: 2.285132\n",
      "Epoch 172, loss: 2.286027\n",
      "Epoch 173, loss: 2.288764\n",
      "Epoch 174, loss: 2.282473\n",
      "Epoch 175, loss: 2.282323\n",
      "Epoch 176, loss: 2.286576\n",
      "Epoch 177, loss: 2.285559\n",
      "Epoch 178, loss: 2.286312\n",
      "Epoch 179, loss: 2.287160\n",
      "Epoch 180, loss: 2.283040\n",
      "Epoch 181, loss: 2.288769\n",
      "Epoch 182, loss: 2.284673\n",
      "Epoch 183, loss: 2.279812\n",
      "Epoch 184, loss: 2.282168\n",
      "Epoch 185, loss: 2.281001\n",
      "Epoch 186, loss: 2.287845\n",
      "Epoch 187, loss: 2.290384\n",
      "Epoch 188, loss: 2.279886\n",
      "Epoch 189, loss: 2.282188\n",
      "Epoch 190, loss: 2.285009\n",
      "Epoch 191, loss: 2.288389\n",
      "Epoch 192, loss: 2.279436\n",
      "Epoch 193, loss: 2.287822\n",
      "Epoch 194, loss: 2.283474\n",
      "Epoch 195, loss: 2.285226\n",
      "Epoch 196, loss: 2.288122\n",
      "Epoch 197, loss: 2.284371\n",
      "Epoch 198, loss: 2.281847\n",
      "Epoch 199, loss: 2.280797\n",
      "Epoch 0, loss: 2.302858\n",
      "Epoch 1, loss: 2.302116\n",
      "Epoch 2, loss: 2.301595\n",
      "Epoch 3, loss: 2.302313\n",
      "Epoch 4, loss: 2.300649\n",
      "Epoch 5, loss: 2.302140\n",
      "Epoch 6, loss: 2.300764\n",
      "Epoch 7, loss: 2.300700\n",
      "Epoch 8, loss: 2.300795\n",
      "Epoch 9, loss: 2.300750\n",
      "Epoch 10, loss: 2.300610\n",
      "Epoch 11, loss: 2.301794\n",
      "Epoch 12, loss: 2.300744\n",
      "Epoch 13, loss: 2.301040\n",
      "Epoch 14, loss: 2.300547\n",
      "Epoch 15, loss: 2.299513\n",
      "Epoch 16, loss: 2.300968\n",
      "Epoch 17, loss: 2.301226\n",
      "Epoch 18, loss: 2.300272\n",
      "Epoch 19, loss: 2.300174\n",
      "Epoch 20, loss: 2.301320\n",
      "Epoch 21, loss: 2.300979\n",
      "Epoch 22, loss: 2.299933\n",
      "Epoch 23, loss: 2.300451\n",
      "Epoch 24, loss: 2.299761\n",
      "Epoch 25, loss: 2.300368\n",
      "Epoch 26, loss: 2.299842\n",
      "Epoch 27, loss: 2.299367\n",
      "Epoch 28, loss: 2.298267\n",
      "Epoch 29, loss: 2.299393\n",
      "Epoch 30, loss: 2.300890\n",
      "Epoch 31, loss: 2.299223\n",
      "Epoch 32, loss: 2.298660\n",
      "Epoch 33, loss: 2.299580\n",
      "Epoch 34, loss: 2.299076\n",
      "Epoch 35, loss: 2.298965\n",
      "Epoch 36, loss: 2.301079\n",
      "Epoch 37, loss: 2.297671\n",
      "Epoch 38, loss: 2.299648\n",
      "Epoch 39, loss: 2.300937\n",
      "Epoch 40, loss: 2.297766\n",
      "Epoch 41, loss: 2.298315\n",
      "Epoch 42, loss: 2.298741\n",
      "Epoch 43, loss: 2.296184\n",
      "Epoch 44, loss: 2.297564\n",
      "Epoch 45, loss: 2.295191\n",
      "Epoch 46, loss: 2.299270\n",
      "Epoch 47, loss: 2.297338\n",
      "Epoch 48, loss: 2.296537\n",
      "Epoch 49, loss: 2.296944\n",
      "Epoch 50, loss: 2.299232\n",
      "Epoch 51, loss: 2.296722\n",
      "Epoch 52, loss: 2.297463\n",
      "Epoch 53, loss: 2.295315\n",
      "Epoch 54, loss: 2.295130\n",
      "Epoch 55, loss: 2.299425\n",
      "Epoch 56, loss: 2.298030\n",
      "Epoch 57, loss: 2.296924\n",
      "Epoch 58, loss: 2.297704\n",
      "Epoch 59, loss: 2.294964\n",
      "Epoch 60, loss: 2.297635\n",
      "Epoch 61, loss: 2.298465\n",
      "Epoch 62, loss: 2.297703\n",
      "Epoch 63, loss: 2.296510\n",
      "Epoch 64, loss: 2.296630\n",
      "Epoch 65, loss: 2.296003\n",
      "Epoch 66, loss: 2.295831\n",
      "Epoch 67, loss: 2.294005\n",
      "Epoch 68, loss: 2.294747\n",
      "Epoch 69, loss: 2.293978\n",
      "Epoch 70, loss: 2.294701\n",
      "Epoch 71, loss: 2.295070\n",
      "Epoch 72, loss: 2.294987\n",
      "Epoch 73, loss: 2.297153\n",
      "Epoch 74, loss: 2.295190\n",
      "Epoch 75, loss: 2.293836\n",
      "Epoch 76, loss: 2.292582\n",
      "Epoch 77, loss: 2.293775\n",
      "Epoch 78, loss: 2.295906\n",
      "Epoch 79, loss: 2.296741\n",
      "Epoch 80, loss: 2.293563\n",
      "Epoch 81, loss: 2.294172\n",
      "Epoch 82, loss: 2.291161\n",
      "Epoch 83, loss: 2.295433\n",
      "Epoch 84, loss: 2.294097\n",
      "Epoch 85, loss: 2.297384\n",
      "Epoch 86, loss: 2.295347\n",
      "Epoch 87, loss: 2.293473\n",
      "Epoch 88, loss: 2.291992\n",
      "Epoch 89, loss: 2.291293\n",
      "Epoch 90, loss: 2.291714\n",
      "Epoch 91, loss: 2.293301\n",
      "Epoch 92, loss: 2.292536\n",
      "Epoch 93, loss: 2.294276\n",
      "Epoch 94, loss: 2.289704\n",
      "Epoch 95, loss: 2.290797\n",
      "Epoch 96, loss: 2.293625\n",
      "Epoch 97, loss: 2.296266\n",
      "Epoch 98, loss: 2.289723\n",
      "Epoch 99, loss: 2.295111\n",
      "Epoch 100, loss: 2.292738\n",
      "Epoch 101, loss: 2.295356\n",
      "Epoch 102, loss: 2.295366\n",
      "Epoch 103, loss: 2.292577\n",
      "Epoch 104, loss: 2.291627\n",
      "Epoch 105, loss: 2.294189\n",
      "Epoch 106, loss: 2.293474\n",
      "Epoch 107, loss: 2.291858\n",
      "Epoch 108, loss: 2.290634\n",
      "Epoch 109, loss: 2.294373\n",
      "Epoch 110, loss: 2.291534\n",
      "Epoch 111, loss: 2.293320\n",
      "Epoch 112, loss: 2.288651\n",
      "Epoch 113, loss: 2.286858\n",
      "Epoch 114, loss: 2.294522\n",
      "Epoch 115, loss: 2.291925\n",
      "Epoch 116, loss: 2.289211\n",
      "Epoch 117, loss: 2.292174\n",
      "Epoch 118, loss: 2.286402\n",
      "Epoch 119, loss: 2.295253\n",
      "Epoch 120, loss: 2.292236\n",
      "Epoch 121, loss: 2.293510\n",
      "Epoch 122, loss: 2.288709\n",
      "Epoch 123, loss: 2.291016\n",
      "Epoch 124, loss: 2.291711\n",
      "Epoch 125, loss: 2.288429\n",
      "Epoch 126, loss: 2.294308\n",
      "Epoch 127, loss: 2.294844\n",
      "Epoch 128, loss: 2.290459\n",
      "Epoch 129, loss: 2.290287\n",
      "Epoch 130, loss: 2.287646\n",
      "Epoch 131, loss: 2.291244\n",
      "Epoch 132, loss: 2.293653\n",
      "Epoch 133, loss: 2.289356\n",
      "Epoch 134, loss: 2.292307\n",
      "Epoch 135, loss: 2.291711\n",
      "Epoch 136, loss: 2.292903\n",
      "Epoch 137, loss: 2.292902\n",
      "Epoch 138, loss: 2.288015\n",
      "Epoch 139, loss: 2.291590\n",
      "Epoch 140, loss: 2.288042\n",
      "Epoch 141, loss: 2.293111\n",
      "Epoch 142, loss: 2.290763\n",
      "Epoch 143, loss: 2.291542\n",
      "Epoch 144, loss: 2.288907\n",
      "Epoch 145, loss: 2.289475\n",
      "Epoch 146, loss: 2.291602\n",
      "Epoch 147, loss: 2.286759\n",
      "Epoch 148, loss: 2.287607\n",
      "Epoch 149, loss: 2.290931\n",
      "Epoch 150, loss: 2.287189\n",
      "Epoch 151, loss: 2.288388\n",
      "Epoch 152, loss: 2.289971\n",
      "Epoch 153, loss: 2.287084\n",
      "Epoch 154, loss: 2.292112\n",
      "Epoch 155, loss: 2.289412\n",
      "Epoch 156, loss: 2.287292\n",
      "Epoch 157, loss: 2.290996\n",
      "Epoch 158, loss: 2.284182\n",
      "Epoch 159, loss: 2.291997\n",
      "Epoch 160, loss: 2.284877\n",
      "Epoch 161, loss: 2.286529\n",
      "Epoch 162, loss: 2.286123\n",
      "Epoch 163, loss: 2.286969\n",
      "Epoch 164, loss: 2.287640\n",
      "Epoch 165, loss: 2.289099\n",
      "Epoch 166, loss: 2.289076\n",
      "Epoch 167, loss: 2.288547\n",
      "Epoch 168, loss: 2.288591\n",
      "Epoch 169, loss: 2.284904\n",
      "Epoch 170, loss: 2.282894\n",
      "Epoch 171, loss: 2.290707\n",
      "Epoch 172, loss: 2.292084\n",
      "Epoch 173, loss: 2.283298\n",
      "Epoch 174, loss: 2.289443\n",
      "Epoch 175, loss: 2.285342\n",
      "Epoch 176, loss: 2.287278\n",
      "Epoch 177, loss: 2.287203\n",
      "Epoch 178, loss: 2.289385\n",
      "Epoch 179, loss: 2.283071\n",
      "Epoch 180, loss: 2.284374\n",
      "Epoch 181, loss: 2.288367\n",
      "Epoch 182, loss: 2.283737\n",
      "Epoch 183, loss: 2.287114\n",
      "Epoch 184, loss: 2.281571\n",
      "Epoch 185, loss: 2.289446\n",
      "Epoch 186, loss: 2.285060\n",
      "Epoch 187, loss: 2.285266\n",
      "Epoch 188, loss: 2.286638\n",
      "Epoch 189, loss: 2.282841\n",
      "Epoch 190, loss: 2.286934\n",
      "Epoch 191, loss: 2.287607\n",
      "Epoch 192, loss: 2.287145\n",
      "Epoch 193, loss: 2.289390\n",
      "Epoch 194, loss: 2.281541\n",
      "Epoch 195, loss: 2.288672\n",
      "Epoch 196, loss: 2.283041\n",
      "Epoch 197, loss: 2.290286\n",
      "Epoch 198, loss: 2.282672\n",
      "Epoch 199, loss: 2.286078\n",
      "Epoch 0, loss: 2.303756\n",
      "Epoch 1, loss: 2.302231\n",
      "Epoch 2, loss: 2.302897\n",
      "Epoch 3, loss: 2.302363\n",
      "Epoch 4, loss: 2.302900\n",
      "Epoch 5, loss: 2.303345\n",
      "Epoch 6, loss: 2.303630\n",
      "Epoch 7, loss: 2.303360\n",
      "Epoch 8, loss: 2.302920\n",
      "Epoch 9, loss: 2.303271\n",
      "Epoch 10, loss: 2.303304\n",
      "Epoch 11, loss: 2.303219\n",
      "Epoch 12, loss: 2.301954\n",
      "Epoch 13, loss: 2.302295\n",
      "Epoch 14, loss: 2.303103\n",
      "Epoch 15, loss: 2.302791\n",
      "Epoch 16, loss: 2.302818\n",
      "Epoch 17, loss: 2.302578\n",
      "Epoch 18, loss: 2.302525\n",
      "Epoch 19, loss: 2.302055\n",
      "Epoch 20, loss: 2.302547\n",
      "Epoch 21, loss: 2.301640\n",
      "Epoch 22, loss: 2.302740\n",
      "Epoch 23, loss: 2.302315\n",
      "Epoch 24, loss: 2.302214\n",
      "Epoch 25, loss: 2.302546\n",
      "Epoch 26, loss: 2.302469\n",
      "Epoch 27, loss: 2.303342\n",
      "Epoch 28, loss: 2.303218\n",
      "Epoch 29, loss: 2.302146\n",
      "Epoch 30, loss: 2.302652\n",
      "Epoch 31, loss: 2.302444\n",
      "Epoch 32, loss: 2.302772\n",
      "Epoch 33, loss: 2.302957\n",
      "Epoch 34, loss: 2.301985\n",
      "Epoch 35, loss: 2.302478\n",
      "Epoch 36, loss: 2.302234\n",
      "Epoch 37, loss: 2.302124\n",
      "Epoch 38, loss: 2.302994\n",
      "Epoch 39, loss: 2.302521\n",
      "Epoch 40, loss: 2.301140\n",
      "Epoch 41, loss: 2.301634\n",
      "Epoch 42, loss: 2.301349\n",
      "Epoch 43, loss: 2.302140\n",
      "Epoch 44, loss: 2.302211\n",
      "Epoch 45, loss: 2.301928\n",
      "Epoch 46, loss: 2.302669\n",
      "Epoch 47, loss: 2.302217\n",
      "Epoch 48, loss: 2.302589\n",
      "Epoch 49, loss: 2.302072\n",
      "Epoch 50, loss: 2.303209\n",
      "Epoch 51, loss: 2.302179\n",
      "Epoch 52, loss: 2.302434\n",
      "Epoch 53, loss: 2.302259\n",
      "Epoch 54, loss: 2.301802\n",
      "Epoch 55, loss: 2.302242\n",
      "Epoch 56, loss: 2.301562\n",
      "Epoch 57, loss: 2.301831\n",
      "Epoch 58, loss: 2.302471\n",
      "Epoch 59, loss: 2.302511\n",
      "Epoch 60, loss: 2.302455\n",
      "Epoch 61, loss: 2.302410\n",
      "Epoch 62, loss: 2.301881\n",
      "Epoch 63, loss: 2.303296\n",
      "Epoch 64, loss: 2.301884\n",
      "Epoch 65, loss: 2.302480\n",
      "Epoch 66, loss: 2.302381\n",
      "Epoch 67, loss: 2.302152\n",
      "Epoch 68, loss: 2.301907\n",
      "Epoch 69, loss: 2.301620\n",
      "Epoch 70, loss: 2.302118\n",
      "Epoch 71, loss: 2.302094\n",
      "Epoch 72, loss: 2.301347\n",
      "Epoch 73, loss: 2.302316\n",
      "Epoch 74, loss: 2.301253\n",
      "Epoch 75, loss: 2.302487\n",
      "Epoch 76, loss: 2.301912\n",
      "Epoch 77, loss: 2.301325\n",
      "Epoch 78, loss: 2.301596\n",
      "Epoch 79, loss: 2.301465\n",
      "Epoch 80, loss: 2.302042\n",
      "Epoch 81, loss: 2.301254\n",
      "Epoch 82, loss: 2.301764\n",
      "Epoch 83, loss: 2.300992\n",
      "Epoch 84, loss: 2.301765\n",
      "Epoch 85, loss: 2.301909\n",
      "Epoch 86, loss: 2.302313\n",
      "Epoch 87, loss: 2.301543\n",
      "Epoch 88, loss: 2.302665\n",
      "Epoch 89, loss: 2.301650\n",
      "Epoch 90, loss: 2.301894\n",
      "Epoch 91, loss: 2.302968\n",
      "Epoch 92, loss: 2.301518\n",
      "Epoch 93, loss: 2.300595\n",
      "Epoch 94, loss: 2.301855\n",
      "Epoch 95, loss: 2.301114\n",
      "Epoch 96, loss: 2.301340\n",
      "Epoch 97, loss: 2.302212\n",
      "Epoch 98, loss: 2.302043\n",
      "Epoch 99, loss: 2.301109\n",
      "Epoch 100, loss: 2.302461\n",
      "Epoch 101, loss: 2.301771\n",
      "Epoch 102, loss: 2.301782\n",
      "Epoch 103, loss: 2.301597\n",
      "Epoch 104, loss: 2.300188\n",
      "Epoch 105, loss: 2.302273\n",
      "Epoch 106, loss: 2.300732\n",
      "Epoch 107, loss: 2.300658\n",
      "Epoch 108, loss: 2.302007\n",
      "Epoch 109, loss: 2.301499\n",
      "Epoch 110, loss: 2.301390\n",
      "Epoch 111, loss: 2.300341\n",
      "Epoch 112, loss: 2.302190\n",
      "Epoch 113, loss: 2.302200\n",
      "Epoch 114, loss: 2.302437\n",
      "Epoch 115, loss: 2.300771\n",
      "Epoch 116, loss: 2.301626\n",
      "Epoch 117, loss: 2.301571\n",
      "Epoch 118, loss: 2.302194\n",
      "Epoch 119, loss: 2.301410\n",
      "Epoch 120, loss: 2.301468\n",
      "Epoch 121, loss: 2.300891\n",
      "Epoch 122, loss: 2.301594\n",
      "Epoch 123, loss: 2.301638\n",
      "Epoch 124, loss: 2.301214\n",
      "Epoch 125, loss: 2.300804\n",
      "Epoch 126, loss: 2.300941\n",
      "Epoch 127, loss: 2.301088\n",
      "Epoch 128, loss: 2.301480\n",
      "Epoch 129, loss: 2.301277\n",
      "Epoch 130, loss: 2.301273\n",
      "Epoch 131, loss: 2.300950\n",
      "Epoch 132, loss: 2.301739\n",
      "Epoch 133, loss: 2.301452\n",
      "Epoch 134, loss: 2.301373\n",
      "Epoch 135, loss: 2.302205\n",
      "Epoch 136, loss: 2.301469\n",
      "Epoch 137, loss: 2.301998\n",
      "Epoch 138, loss: 2.301479\n",
      "Epoch 139, loss: 2.300775\n",
      "Epoch 140, loss: 2.300933\n",
      "Epoch 141, loss: 2.301717\n",
      "Epoch 142, loss: 2.302391\n",
      "Epoch 143, loss: 2.300857\n",
      "Epoch 144, loss: 2.301292\n",
      "Epoch 145, loss: 2.300930\n",
      "Epoch 146, loss: 2.300780\n",
      "Epoch 147, loss: 2.300894\n",
      "Epoch 148, loss: 2.301543\n",
      "Epoch 149, loss: 2.301253\n",
      "Epoch 150, loss: 2.301467\n",
      "Epoch 151, loss: 2.301316\n",
      "Epoch 152, loss: 2.301253\n",
      "Epoch 153, loss: 2.300949\n",
      "Epoch 154, loss: 2.301552\n",
      "Epoch 155, loss: 2.300090\n",
      "Epoch 156, loss: 2.300117\n",
      "Epoch 157, loss: 2.301951\n",
      "Epoch 158, loss: 2.300768\n",
      "Epoch 159, loss: 2.300333\n",
      "Epoch 160, loss: 2.300756\n",
      "Epoch 161, loss: 2.301340\n",
      "Epoch 162, loss: 2.302103\n",
      "Epoch 163, loss: 2.300570\n",
      "Epoch 164, loss: 2.300818\n",
      "Epoch 165, loss: 2.300690\n",
      "Epoch 166, loss: 2.301501\n",
      "Epoch 167, loss: 2.301028\n",
      "Epoch 168, loss: 2.300305\n",
      "Epoch 169, loss: 2.300642\n",
      "Epoch 170, loss: 2.300368\n",
      "Epoch 171, loss: 2.299407\n",
      "Epoch 172, loss: 2.301883\n",
      "Epoch 173, loss: 2.301325\n",
      "Epoch 174, loss: 2.301553\n",
      "Epoch 175, loss: 2.301543\n",
      "Epoch 176, loss: 2.301527\n",
      "Epoch 177, loss: 2.300510\n",
      "Epoch 178, loss: 2.300592\n",
      "Epoch 179, loss: 2.301126\n",
      "Epoch 180, loss: 2.301652\n",
      "Epoch 181, loss: 2.300362\n",
      "Epoch 182, loss: 2.299831\n",
      "Epoch 183, loss: 2.300033\n",
      "Epoch 184, loss: 2.299508\n",
      "Epoch 185, loss: 2.299659\n",
      "Epoch 186, loss: 2.301495\n",
      "Epoch 187, loss: 2.301892\n",
      "Epoch 188, loss: 2.300460\n",
      "Epoch 189, loss: 2.300816\n",
      "Epoch 190, loss: 2.300293\n",
      "Epoch 191, loss: 2.300057\n",
      "Epoch 192, loss: 2.300816\n",
      "Epoch 193, loss: 2.301244\n",
      "Epoch 194, loss: 2.301085\n",
      "Epoch 195, loss: 2.300595\n",
      "Epoch 196, loss: 2.299911\n",
      "Epoch 197, loss: 2.299409\n",
      "Epoch 198, loss: 2.300056\n",
      "Epoch 199, loss: 2.301386\n",
      "Epoch 0, loss: 2.302136\n",
      "Epoch 1, loss: 2.302063\n",
      "Epoch 2, loss: 2.302097\n",
      "Epoch 3, loss: 2.302193\n",
      "Epoch 4, loss: 2.301875\n",
      "Epoch 5, loss: 2.302004\n",
      "Epoch 6, loss: 2.303297\n",
      "Epoch 7, loss: 2.303182\n",
      "Epoch 8, loss: 2.302106\n",
      "Epoch 9, loss: 2.302425\n",
      "Epoch 10, loss: 2.302929\n",
      "Epoch 11, loss: 2.304059\n",
      "Epoch 12, loss: 2.303546\n",
      "Epoch 13, loss: 2.302980\n",
      "Epoch 14, loss: 2.302754\n",
      "Epoch 15, loss: 2.302764\n",
      "Epoch 16, loss: 2.301993\n",
      "Epoch 17, loss: 2.303138\n",
      "Epoch 18, loss: 2.302323\n",
      "Epoch 19, loss: 2.302066\n",
      "Epoch 20, loss: 2.303106\n",
      "Epoch 21, loss: 2.302148\n",
      "Epoch 22, loss: 2.301314\n",
      "Epoch 23, loss: 2.302103\n",
      "Epoch 24, loss: 2.302190\n",
      "Epoch 25, loss: 2.303771\n",
      "Epoch 26, loss: 2.302701\n",
      "Epoch 27, loss: 2.301833\n",
      "Epoch 28, loss: 2.303011\n",
      "Epoch 29, loss: 2.301961\n",
      "Epoch 30, loss: 2.302674\n",
      "Epoch 31, loss: 2.301912\n",
      "Epoch 32, loss: 2.301388\n",
      "Epoch 33, loss: 2.301721\n",
      "Epoch 34, loss: 2.301066\n",
      "Epoch 35, loss: 2.302158\n",
      "Epoch 36, loss: 2.301535\n",
      "Epoch 37, loss: 2.301532\n",
      "Epoch 38, loss: 2.301996\n",
      "Epoch 39, loss: 2.302273\n",
      "Epoch 40, loss: 2.302048\n",
      "Epoch 41, loss: 2.301700\n",
      "Epoch 42, loss: 2.302293\n",
      "Epoch 43, loss: 2.301862\n",
      "Epoch 44, loss: 2.302210\n",
      "Epoch 45, loss: 2.302477\n",
      "Epoch 46, loss: 2.302039\n",
      "Epoch 47, loss: 2.302930\n",
      "Epoch 48, loss: 2.303152\n",
      "Epoch 49, loss: 2.301706\n",
      "Epoch 50, loss: 2.301553\n",
      "Epoch 51, loss: 2.301089\n",
      "Epoch 52, loss: 2.303705\n",
      "Epoch 53, loss: 2.302265\n",
      "Epoch 54, loss: 2.301793\n",
      "Epoch 55, loss: 2.302035\n",
      "Epoch 56, loss: 2.302886\n",
      "Epoch 57, loss: 2.301271\n",
      "Epoch 58, loss: 2.301373\n",
      "Epoch 59, loss: 2.301914\n",
      "Epoch 60, loss: 2.301457\n",
      "Epoch 61, loss: 2.301291\n",
      "Epoch 62, loss: 2.301164\n",
      "Epoch 63, loss: 2.301899\n",
      "Epoch 64, loss: 2.301448\n",
      "Epoch 65, loss: 2.302024\n",
      "Epoch 66, loss: 2.302425\n",
      "Epoch 67, loss: 2.302260\n",
      "Epoch 68, loss: 2.301266\n",
      "Epoch 69, loss: 2.301696\n",
      "Epoch 70, loss: 2.300638\n",
      "Epoch 71, loss: 2.302260\n",
      "Epoch 72, loss: 2.302723\n",
      "Epoch 73, loss: 2.301178\n",
      "Epoch 74, loss: 2.300812\n",
      "Epoch 75, loss: 2.302220\n",
      "Epoch 76, loss: 2.301738\n",
      "Epoch 77, loss: 2.301053\n",
      "Epoch 78, loss: 2.302161\n",
      "Epoch 79, loss: 2.300865\n",
      "Epoch 80, loss: 2.301367\n",
      "Epoch 81, loss: 2.302369\n",
      "Epoch 82, loss: 2.303273\n",
      "Epoch 83, loss: 2.301722\n",
      "Epoch 84, loss: 2.302075\n",
      "Epoch 85, loss: 2.301507\n",
      "Epoch 86, loss: 2.301536\n",
      "Epoch 87, loss: 2.301191\n",
      "Epoch 88, loss: 2.300779\n",
      "Epoch 89, loss: 2.303102\n",
      "Epoch 90, loss: 2.302059\n",
      "Epoch 91, loss: 2.301541\n",
      "Epoch 92, loss: 2.301607\n",
      "Epoch 93, loss: 2.300593\n",
      "Epoch 94, loss: 2.302255\n",
      "Epoch 95, loss: 2.301811\n",
      "Epoch 96, loss: 2.301503\n",
      "Epoch 97, loss: 2.301880\n",
      "Epoch 98, loss: 2.300369\n",
      "Epoch 99, loss: 2.301925\n",
      "Epoch 100, loss: 2.302353\n",
      "Epoch 101, loss: 2.300920\n",
      "Epoch 102, loss: 2.301420\n",
      "Epoch 103, loss: 2.301892\n",
      "Epoch 104, loss: 2.302077\n",
      "Epoch 105, loss: 2.301090\n",
      "Epoch 106, loss: 2.301676\n",
      "Epoch 107, loss: 2.300871\n",
      "Epoch 108, loss: 2.302280\n",
      "Epoch 109, loss: 2.302318\n",
      "Epoch 110, loss: 2.301349\n",
      "Epoch 111, loss: 2.301389\n",
      "Epoch 112, loss: 2.301878\n",
      "Epoch 113, loss: 2.301568\n",
      "Epoch 114, loss: 2.300154\n",
      "Epoch 115, loss: 2.301015\n",
      "Epoch 116, loss: 2.300820\n",
      "Epoch 117, loss: 2.300883\n",
      "Epoch 118, loss: 2.302166\n",
      "Epoch 119, loss: 2.302264\n",
      "Epoch 120, loss: 2.301587\n",
      "Epoch 121, loss: 2.301760\n",
      "Epoch 122, loss: 2.302019\n",
      "Epoch 123, loss: 2.300951\n",
      "Epoch 124, loss: 2.300905\n",
      "Epoch 125, loss: 2.301553\n",
      "Epoch 126, loss: 2.299846\n",
      "Epoch 127, loss: 2.300478\n",
      "Epoch 128, loss: 2.302392\n",
      "Epoch 129, loss: 2.299215\n",
      "Epoch 130, loss: 2.301521\n",
      "Epoch 131, loss: 2.301744\n",
      "Epoch 132, loss: 2.301198\n",
      "Epoch 133, loss: 2.301147\n",
      "Epoch 134, loss: 2.300695\n",
      "Epoch 135, loss: 2.302028\n",
      "Epoch 136, loss: 2.301339\n",
      "Epoch 137, loss: 2.301076\n",
      "Epoch 138, loss: 2.300834\n",
      "Epoch 139, loss: 2.300400\n",
      "Epoch 140, loss: 2.300644\n",
      "Epoch 141, loss: 2.299533\n",
      "Epoch 142, loss: 2.301485\n",
      "Epoch 143, loss: 2.300110\n",
      "Epoch 144, loss: 2.301668\n",
      "Epoch 145, loss: 2.301592\n",
      "Epoch 146, loss: 2.301361\n",
      "Epoch 147, loss: 2.301453\n",
      "Epoch 148, loss: 2.301149\n",
      "Epoch 149, loss: 2.301522\n",
      "Epoch 150, loss: 2.301012\n",
      "Epoch 151, loss: 2.300794\n",
      "Epoch 152, loss: 2.301072\n",
      "Epoch 153, loss: 2.300552\n",
      "Epoch 154, loss: 2.301955\n",
      "Epoch 155, loss: 2.301708\n",
      "Epoch 156, loss: 2.300032\n",
      "Epoch 157, loss: 2.300087\n",
      "Epoch 158, loss: 2.300235\n",
      "Epoch 159, loss: 2.301946\n",
      "Epoch 160, loss: 2.300753\n",
      "Epoch 161, loss: 2.300619\n",
      "Epoch 162, loss: 2.300581\n",
      "Epoch 163, loss: 2.301195\n",
      "Epoch 164, loss: 2.298567\n",
      "Epoch 165, loss: 2.302769\n",
      "Epoch 166, loss: 2.301068\n",
      "Epoch 167, loss: 2.300198\n",
      "Epoch 168, loss: 2.301429\n",
      "Epoch 169, loss: 2.300128\n",
      "Epoch 170, loss: 2.298825\n",
      "Epoch 171, loss: 2.301406\n",
      "Epoch 172, loss: 2.301772\n",
      "Epoch 173, loss: 2.302464\n",
      "Epoch 174, loss: 2.300687\n",
      "Epoch 175, loss: 2.301035\n",
      "Epoch 176, loss: 2.300077\n",
      "Epoch 177, loss: 2.301863\n",
      "Epoch 178, loss: 2.299909\n",
      "Epoch 179, loss: 2.299802\n",
      "Epoch 180, loss: 2.300463\n",
      "Epoch 181, loss: 2.299943\n",
      "Epoch 182, loss: 2.300718\n",
      "Epoch 183, loss: 2.299823\n",
      "Epoch 184, loss: 2.299228\n",
      "Epoch 185, loss: 2.300182\n",
      "Epoch 186, loss: 2.300156\n",
      "Epoch 187, loss: 2.299495\n",
      "Epoch 188, loss: 2.299825\n",
      "Epoch 189, loss: 2.300231\n",
      "Epoch 190, loss: 2.300475\n",
      "Epoch 191, loss: 2.300023\n",
      "Epoch 192, loss: 2.301066\n",
      "Epoch 193, loss: 2.299202\n",
      "Epoch 194, loss: 2.300747\n",
      "Epoch 195, loss: 2.300667\n",
      "Epoch 196, loss: 2.301707\n",
      "Epoch 197, loss: 2.299782\n",
      "Epoch 198, loss: 2.300290\n",
      "Epoch 199, loss: 2.301882\n",
      "Epoch 0, loss: 2.302488\n",
      "Epoch 1, loss: 2.302668\n",
      "Epoch 2, loss: 2.301558\n",
      "Epoch 3, loss: 2.302353\n",
      "Epoch 4, loss: 2.303242\n",
      "Epoch 5, loss: 2.302640\n",
      "Epoch 6, loss: 2.302925\n",
      "Epoch 7, loss: 2.303245\n",
      "Epoch 8, loss: 2.302860\n",
      "Epoch 9, loss: 2.301845\n",
      "Epoch 10, loss: 2.302271\n",
      "Epoch 11, loss: 2.302665\n",
      "Epoch 12, loss: 2.303139\n",
      "Epoch 13, loss: 2.302695\n",
      "Epoch 14, loss: 2.303667\n",
      "Epoch 15, loss: 2.303463\n",
      "Epoch 16, loss: 2.302286\n",
      "Epoch 17, loss: 2.301834\n",
      "Epoch 18, loss: 2.302311\n",
      "Epoch 19, loss: 2.302096\n",
      "Epoch 20, loss: 2.302144\n",
      "Epoch 21, loss: 2.301826\n",
      "Epoch 22, loss: 2.303153\n",
      "Epoch 23, loss: 2.302193\n",
      "Epoch 24, loss: 2.302617\n",
      "Epoch 25, loss: 2.302409\n",
      "Epoch 26, loss: 2.303168\n",
      "Epoch 27, loss: 2.302254\n",
      "Epoch 28, loss: 2.302399\n",
      "Epoch 29, loss: 2.303790\n",
      "Epoch 30, loss: 2.302436\n",
      "Epoch 31, loss: 2.301773\n",
      "Epoch 32, loss: 2.303545\n",
      "Epoch 33, loss: 2.302176\n",
      "Epoch 34, loss: 2.303002\n",
      "Epoch 35, loss: 2.302249\n",
      "Epoch 36, loss: 2.301868\n",
      "Epoch 37, loss: 2.301980\n",
      "Epoch 38, loss: 2.303235\n",
      "Epoch 39, loss: 2.301966\n",
      "Epoch 40, loss: 2.301662\n",
      "Epoch 41, loss: 2.302431\n",
      "Epoch 42, loss: 2.302754\n",
      "Epoch 43, loss: 2.302911\n",
      "Epoch 44, loss: 2.301210\n",
      "Epoch 45, loss: 2.302760\n",
      "Epoch 46, loss: 2.303102\n",
      "Epoch 47, loss: 2.301627\n",
      "Epoch 48, loss: 2.303002\n",
      "Epoch 49, loss: 2.302480\n",
      "Epoch 50, loss: 2.302283\n",
      "Epoch 51, loss: 2.302817\n",
      "Epoch 52, loss: 2.302105\n",
      "Epoch 53, loss: 2.301548\n",
      "Epoch 54, loss: 2.302336\n",
      "Epoch 55, loss: 2.302768\n",
      "Epoch 56, loss: 2.301633\n",
      "Epoch 57, loss: 2.302026\n",
      "Epoch 58, loss: 2.302554\n",
      "Epoch 59, loss: 2.303449\n",
      "Epoch 60, loss: 2.302689\n",
      "Epoch 61, loss: 2.302346\n",
      "Epoch 62, loss: 2.303643\n",
      "Epoch 63, loss: 2.302438\n",
      "Epoch 64, loss: 2.303382\n",
      "Epoch 65, loss: 2.302468\n",
      "Epoch 66, loss: 2.302466\n",
      "Epoch 67, loss: 2.302270\n",
      "Epoch 68, loss: 2.301629\n",
      "Epoch 69, loss: 2.300944\n",
      "Epoch 70, loss: 2.301551\n",
      "Epoch 71, loss: 2.302132\n",
      "Epoch 72, loss: 2.302594\n",
      "Epoch 73, loss: 2.302023\n",
      "Epoch 74, loss: 2.301257\n",
      "Epoch 75, loss: 2.302819\n",
      "Epoch 76, loss: 2.301148\n",
      "Epoch 77, loss: 2.301773\n",
      "Epoch 78, loss: 2.302345\n",
      "Epoch 79, loss: 2.301132\n",
      "Epoch 80, loss: 2.302709\n",
      "Epoch 81, loss: 2.301916\n",
      "Epoch 82, loss: 2.301018\n",
      "Epoch 83, loss: 2.301386\n",
      "Epoch 84, loss: 2.302325\n",
      "Epoch 85, loss: 2.300919\n",
      "Epoch 86, loss: 2.301148\n",
      "Epoch 87, loss: 2.300603\n",
      "Epoch 88, loss: 2.301195\n",
      "Epoch 89, loss: 2.301512\n",
      "Epoch 90, loss: 2.301979\n",
      "Epoch 91, loss: 2.302531\n",
      "Epoch 92, loss: 2.301806\n",
      "Epoch 93, loss: 2.300899\n",
      "Epoch 94, loss: 2.302049\n",
      "Epoch 95, loss: 2.301929\n",
      "Epoch 96, loss: 2.301458\n",
      "Epoch 97, loss: 2.301681\n",
      "Epoch 98, loss: 2.301026\n",
      "Epoch 99, loss: 2.302995\n",
      "Epoch 100, loss: 2.301351\n",
      "Epoch 101, loss: 2.303162\n",
      "Epoch 102, loss: 2.301272\n",
      "Epoch 103, loss: 2.302115\n",
      "Epoch 104, loss: 2.300868\n",
      "Epoch 105, loss: 2.301456\n",
      "Epoch 106, loss: 2.302159\n",
      "Epoch 107, loss: 2.302704\n",
      "Epoch 108, loss: 2.301624\n",
      "Epoch 109, loss: 2.301623\n",
      "Epoch 110, loss: 2.301934\n",
      "Epoch 111, loss: 2.300734\n",
      "Epoch 112, loss: 2.302139\n",
      "Epoch 113, loss: 2.301621\n",
      "Epoch 114, loss: 2.302567\n",
      "Epoch 115, loss: 2.302031\n",
      "Epoch 116, loss: 2.300727\n",
      "Epoch 117, loss: 2.302123\n",
      "Epoch 118, loss: 2.302091\n",
      "Epoch 119, loss: 2.301351\n",
      "Epoch 120, loss: 2.300689\n",
      "Epoch 121, loss: 2.301636\n",
      "Epoch 122, loss: 2.301454\n",
      "Epoch 123, loss: 2.300708\n",
      "Epoch 124, loss: 2.300442\n",
      "Epoch 125, loss: 2.301565\n",
      "Epoch 126, loss: 2.300231\n",
      "Epoch 127, loss: 2.301414\n",
      "Epoch 128, loss: 2.300669\n",
      "Epoch 129, loss: 2.301532\n",
      "Epoch 130, loss: 2.301288\n",
      "Epoch 131, loss: 2.301666\n",
      "Epoch 132, loss: 2.301555\n",
      "Epoch 133, loss: 2.300463\n",
      "Epoch 134, loss: 2.301926\n",
      "Epoch 135, loss: 2.301443\n",
      "Epoch 136, loss: 2.302068\n",
      "Epoch 137, loss: 2.300570\n",
      "Epoch 138, loss: 2.301013\n",
      "Epoch 139, loss: 2.300749\n",
      "Epoch 140, loss: 2.301389\n",
      "Epoch 141, loss: 2.301911\n",
      "Epoch 142, loss: 2.301437\n",
      "Epoch 143, loss: 2.301316\n",
      "Epoch 144, loss: 2.301451\n",
      "Epoch 145, loss: 2.301771\n",
      "Epoch 146, loss: 2.301169\n",
      "Epoch 147, loss: 2.301846\n",
      "Epoch 148, loss: 2.301238\n",
      "Epoch 149, loss: 2.302289\n",
      "Epoch 150, loss: 2.300232\n",
      "Epoch 151, loss: 2.300817\n",
      "Epoch 152, loss: 2.301271\n",
      "Epoch 153, loss: 2.301517\n",
      "Epoch 154, loss: 2.302699\n",
      "Epoch 155, loss: 2.300962\n",
      "Epoch 156, loss: 2.300561\n",
      "Epoch 157, loss: 2.301002\n",
      "Epoch 158, loss: 2.301781\n",
      "Epoch 159, loss: 2.300571\n",
      "Epoch 160, loss: 2.300718\n",
      "Epoch 161, loss: 2.300655\n",
      "Epoch 162, loss: 2.301301\n",
      "Epoch 163, loss: 2.300319\n",
      "Epoch 164, loss: 2.301772\n",
      "Epoch 165, loss: 2.299693\n",
      "Epoch 166, loss: 2.300259\n",
      "Epoch 167, loss: 2.299903\n",
      "Epoch 168, loss: 2.302217\n",
      "Epoch 169, loss: 2.300636\n",
      "Epoch 170, loss: 2.300673\n",
      "Epoch 171, loss: 2.300635\n",
      "Epoch 172, loss: 2.301892\n",
      "Epoch 173, loss: 2.300153\n",
      "Epoch 174, loss: 2.300285\n",
      "Epoch 175, loss: 2.301335\n",
      "Epoch 176, loss: 2.300161\n",
      "Epoch 177, loss: 2.302082\n",
      "Epoch 178, loss: 2.299539\n",
      "Epoch 179, loss: 2.300999\n",
      "Epoch 180, loss: 2.299778\n",
      "Epoch 181, loss: 2.301107\n",
      "Epoch 182, loss: 2.301724\n",
      "Epoch 183, loss: 2.300619\n",
      "Epoch 184, loss: 2.301465\n",
      "Epoch 185, loss: 2.302565\n",
      "Epoch 186, loss: 2.299868\n",
      "Epoch 187, loss: 2.301396\n",
      "Epoch 188, loss: 2.300572\n",
      "Epoch 189, loss: 2.301121\n",
      "Epoch 190, loss: 2.301882\n",
      "Epoch 191, loss: 2.301751\n",
      "Epoch 192, loss: 2.300257\n",
      "Epoch 193, loss: 2.301492\n",
      "Epoch 194, loss: 2.300729\n",
      "Epoch 195, loss: 2.301139\n",
      "Epoch 196, loss: 2.300323\n",
      "Epoch 197, loss: 2.301402\n",
      "Epoch 198, loss: 2.300555\n",
      "Epoch 199, loss: 2.299162\n",
      "best validation accuracy achieved: 0.228000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                                      learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_lr, best_rs = lr, rs\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.196000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
